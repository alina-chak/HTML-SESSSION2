{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "_Improved_SBD_Gygli.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alina-chak/HTML-SESSSION2/blob/master/_Improved_SBD_Gygli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2LuU7upaOoX",
        "outputId": "fc20ae18-7df7-43a9-ed47-b6a2626b4dee"
      },
      "source": [
        "#OPENCV VERSION FOR USING SIFT\n",
        "!pip install opencv-python==3.4.2.17\n",
        "!pip install opencv-contrib-python==3.4.2.17\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==3.4.2.17) (1.19.5)\n",
            "Requirement already satisfied: opencv-contrib-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==3.4.2.17) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3LK5Wiw6ZUH",
        "outputId": "00548a8a-9d52-4369-f5e8-3f04afc38655"
      },
      "source": [
        "!git clone https://github.com/oladeha2/shot_boudary_detector.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'shot_boudary_detector'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Total 64 (delta 0), reused 0 (delta 0), pack-reused 64\u001b[K\n",
            "Unpacking objects: 100% (64/64), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScEUbTMLoDHZ",
        "outputId": "0a20a3d7-a56a-4414-f715-e6ac76ad4d50"
      },
      "source": [
        "%cd /content/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "sample_data  shot_boudary_detector\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lh11MGB57Xxe",
        "outputId": "e27c1abd-06b1-4fc4-a7cd-eb80cd072264"
      },
      "source": [
        "%cd /content/shot_boudary_detector\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/shot_boudary_detector\n",
            "get_transition_frames_cpu.py\t\tTestVideo.py\n",
            "get_transition_frames_gpu.py\t\ttransition_network.py\n",
            "README.md\t\t\t\tutilities.py\n",
            "shot_boundary_detector_even_distrib.pt\tvideo_processing.py\n",
            "snippet.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4wu_tdgvWxh",
        "outputId": "09776e59-17b5-49d9-cf26-b7f188e60e5e"
      },
      "source": [
        "!python image_save.py /content/v1.mp4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decomposing video to frames this may take a while  for large videos :) .....\n",
            "final size:  (64, 64)\n",
            "frame decomposition complete !!! \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziaM8PFHSslf"
      },
      "source": [
        "import sys\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import numpy as np\n",
        "from snippet import getSnippet\n",
        "from math import floor\n",
        "from transition_network import TransitionCNN\n",
        "from utilities import normalize_frame, print_shape\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from TestVideo import TestVideo, return_start_and_end\n",
        "from moviepy.editor import VideoFileClip\n",
        "from video_processing import six_four_crop_video\n",
        "from PIL import Image\n",
        "\n",
        "# command line arguments --> file name, video_file_name, gpu or cpu \n",
        "\n",
        "\n",
        "# first decompose the video to frames\n",
        "# place the video to be detected into the directory \n",
        "\n",
        "video = sys.argv[1]\n",
        "pred_text_file_name = sys.argv[2]\n",
        "\n",
        "\n",
        "text_file = 'frames.txt'\n",
        "\n",
        "print('decomposing video to frames this may take a while  for large videos :) .....')\n",
        "frames_path = 'video_frames/'\n",
        "os.makedirs('video_frames/', exist_ok=True)\n",
        "os.makedirs('predictions/', exist_ok=True)\n",
        "\n",
        "vid = VideoFileClip(video)\n",
        "vid = six_four_crop_video(vid)\n",
        "\n",
        "frames = [frame for frame in vid.iter_frames()]\n",
        "\n",
        "f = open(text_file, 'w+')\n",
        "\n",
        "for j, frame in enumerate(frames):\n",
        "        frame_path = frames_path + 'frame_' + str(j+1) + '.png'\n",
        "        im = Image.fromarray(frame)\n",
        "        im.save(frame_path)            \n",
        "        f.write(frame_path + '\\n')    \n",
        "\n",
        "print('frame decomposition complete !!! ')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh0_Ve3zo5yr",
        "outputId": "4c1c2210-ff7d-4b28-d48e-2e8671a7d134"
      },
      "source": [
        "#CODE FOR GETTING NO OF FRAMES\n",
        "import cv2\n",
        "cap= cv2.VideoCapture('/content/v1.mp4')\n",
        "\n",
        "totalframecount= int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "print(\"The total number of frames in this video is \", totalframecount)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The total number of frames in this video is  14541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cl5wKQGJfIF"
      },
      "source": [
        "#CODE FOR EXTRACTING VIDEO FRAMES\n",
        "import cv2\n",
        "\n",
        "# Opens the Video file\n",
        "cap= cv2.VideoCapture('/content/v1.mp4')\n",
        "i=0\n",
        "while(cap.isOpened()):\n",
        "    ret, frame = cap.read()\n",
        "    if ret == False:\n",
        "        break\n",
        "    cv2.imwrite('kang'+str(i)+'.jpg',frame)\n",
        "    i+=1\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeRygES_KC0m"
      },
      "source": [
        "#CHANGES IN CODE FOR FALSE PREDICTION REMOVAL\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import numpy as np\n",
        "from snippet import getSnippet\n",
        "from math import floor\n",
        "from transition_network import TransitionCNN\n",
        "from utilities import normalize_frame, print_shape\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from TestVideo import TestVideo, return_start_and_end\n",
        "from moviepy.editor import VideoFileClip\n",
        "from video_processing import six_four_crop_video\n",
        "from PIL import Image\n",
        "import cv2 \n",
        "#import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "\n",
        "# command line arguments --> file name, video_file_name, gpu or cpu \n",
        "\n",
        "\n",
        "# first decompose the video to frames\n",
        "# place the video to be detected into the directory \n",
        "\n",
        "video = sys.argv[1]\n",
        "pred_text_file_name = sys.argv[2]\n",
        "\n",
        "\n",
        "text_file = 'frames.txt'\n",
        "\n",
        "print('decomposing video to frames this may take a while  for large videos :) .....')\n",
        "frames_path = 'video_frames/'\n",
        "os.makedirs('video_frames/', exist_ok=True)\n",
        "os.makedirs('predictions/', exist_ok=True)\n",
        "\n",
        "vid = VideoFileClip(video)\n",
        "vid = six_four_crop_video(vid)\n",
        "\n",
        "frames = [frame for frame in vid.iter_frames()]\n",
        "\n",
        "f = open(text_file, 'w+')\n",
        "\n",
        "for j, frame in enumerate(frames):\n",
        "        frame_path = frames_path + 'frame_' + str(j+1) + '.png'\n",
        "        im = Image.fromarray(frame)\n",
        "        im.save(frame_path)            \n",
        "        f.write(frame_path + '\\n')    \n",
        "\n",
        "print('frame decomposition complete !!! ')\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "#load model\n",
        "model = TransitionCNN()\n",
        "model.load_state_dict(torch.load('shot_boundary_detector_even_distrib.pt'))\n",
        "model.to(device)\n",
        "\n",
        "prediction_text_file = 'predictions/' + pred_text_file_name \n",
        "\n",
        "pred_file = open(prediction_text_file, 'w+')\n",
        "\n",
        "print('computing predictions for video', video, '...................' )\n",
        "\n",
        "test_video = TestVideo('frames.txt', sample_size=100, overlap=9)\n",
        "test_loader = DataLoader(test_video, batch_size=1, num_workers=1)\n",
        "\n",
        "video_indexes = []\n",
        "vals = np.arange(test_video.get_line_number())\n",
        "length = len(test_video)\n",
        "\n",
        "for val in range(length):\n",
        "    s,e = return_start_and_end(val)\n",
        "    video_indexes.append(vals[s:e])\n",
        "\n",
        "for indx, batch in enumerate(test_loader):\n",
        "        batch.to(device)\n",
        "        batch = batch.type('torch.cuda.FloatTensor')\n",
        "        predictions = model(batch)\n",
        "        predictions = predictions.argmax(dim=1).cpu().numpy()\n",
        "        for idx, prediction_set in enumerate(predictions):\n",
        "            for i, prediction in enumerate(prediction_set):\n",
        "                if prediction[0][0] == 0:\n",
        "                    frame_index = video_indexes[indx][i+5]\n",
        "                    #print(frame_index)\n",
        "                    frame_path0 = frames_path + 'frame_' + str(int(frame_index)-1) + '.png'\n",
        "                    frame_path = frames_path + 'frame_' + str(frame_index) + '.png'\n",
        "                    #print(frame_path0)\n",
        "                    # read images\n",
        "                    img1 = cv2.imread(frame_path0)  \n",
        "                    img2 = cv2.imread(frame_path) \n",
        "\n",
        "                    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "                    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                    #sift\n",
        "                    sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "                    keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\n",
        "                    keypoints_2, descriptors_2 = sift.detectAndCompute(img2,None)\n",
        "                    a = max()\n",
        "                    print(len(keypoints_1))\n",
        "                    #feature matching\n",
        "                    bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
        "\n",
        "                    matches = bf.match(descriptors_1,descriptors_2)\n",
        "                    #print(len(matches))\n",
        "                    #matches = sorted(matches, key = lambda x:x.distance)\n",
        "                    #print(len(matches))\n",
        "                    print(len(matches)/len(keypoints_1))\n",
        "                    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    \n",
        "                    pred_file.write(str(frame_index) + '\\n')\n",
        "pred_file.close()\n",
        "\n",
        "# delete files used for process\n",
        "os.remove('frames.txt')\n",
        "shutil.rmtree('video_frames/')\n",
        "\n",
        "print('Predictions complete !!!')\n",
        "print('Frames that are part of shot boundaries are listed in file the directory path predictions/' + pred_text_file_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMFcWdvKSEh8"
      },
      "source": [
        "!python get_transition_frames_gpu.py /content/v1.mp4 pred1.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqjWTy3J-z9f"
      },
      "source": [
        "#using SURF features\n",
        "!python get_transition_frames_gpu.py /content/v3.mp4 pred2.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "decEU4FOZXE-"
      },
      "source": [
        "import cv2 \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "fr0 = '/content/messi_3.jfif'\n",
        "fr1 = '/content/messi_4.jfif'\n",
        "# read images\n",
        "img1 = cv2.imread(fr0)  \n",
        "img2 = cv2.imread(fr1) \n",
        "\n",
        "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "#sift\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\n",
        "keypoints_2, descriptors_2 = sift.detectAndCompute(img2,None)\n",
        "print(len(keypoints_1))\n",
        "print(len(keypoints_2))\n",
        "#feature matching\n",
        "bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
        "\n",
        "matches = bf.match(descriptors_1,descriptors_2)\n",
        "\n",
        "print(len(matches))\n",
        "\n",
        "print(len(matches)/len(keypoints_1))\n",
        "\n",
        "#img3 = cv2.drawMatches(img1, keypoints_1, img2, keypoints_2, matches[:50], img2, flags=2)\n",
        "#plt.imshow(img3),plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7EsjyoRt13J"
      },
      "source": [
        "#CHANGES IN CODE FOR FALSE PREDICTION REMOVAL KEYPOINT TESTING\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import numpy as np\n",
        "from snippet import getSnippet\n",
        "from math import floor\n",
        "from transition_network import TransitionCNN\n",
        "from utilities import normalize_frame, print_shape\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from TestVideo import TestVideo, return_start_and_end\n",
        "from moviepy.editor import VideoFileClip\n",
        "from video_processing import six_four_crop_video\n",
        "from PIL import Image\n",
        "import cv2 \n",
        "#import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "\n",
        "# command line arguments --> file name, video_file_name, gpu or cpu \n",
        "\n",
        "\n",
        "# first decompose the video to frames\n",
        "# place the video to be detected into the directory \n",
        "\n",
        "video = sys.argv[1]\n",
        "pred_text_file_name = sys.argv[2]\n",
        "\n",
        "frames_path = '/content/shot_boudary_detector/video_frames/'\n",
        "\n",
        "\n",
        "text_file = 'frames.txt'\n",
        "\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "#load model\n",
        "model = TransitionCNN()\n",
        "model.load_state_dict(torch.load('shot_boundary_detector_even_distrib.pt'))\n",
        "model.to(device)\n",
        "\n",
        "prediction_text_file = 'predictions/' + pred_text_file_name \n",
        "\n",
        "pred_file = open(prediction_text_file, 'w+')\n",
        "\n",
        "print('computing predictions for video', video, '...................' )\n",
        "\n",
        "test_video = TestVideo('frames.txt', sample_size=100, overlap=9)\n",
        "test_loader = DataLoader(test_video, batch_size=1, num_workers=1)\n",
        "\n",
        "video_indexes = []\n",
        "vals = np.arange(test_video.get_line_number())\n",
        "length = len(test_video)\n",
        "\n",
        "for val in range(length):\n",
        "    s,e = return_start_and_end(val)\n",
        "    video_indexes.append(vals[s:e])\n",
        "\n",
        "for indx, batch in enumerate(test_loader):\n",
        "        batch.to(device)\n",
        "        batch = batch.type('torch.cuda.FloatTensor')\n",
        "        predictions = model(batch)\n",
        "        predictions = predictions.argmax(dim=1).cpu().numpy()\n",
        "        for idx, prediction_set in enumerate(predictions):\n",
        "            for i, prediction in enumerate(prediction_set):\n",
        "                if prediction[0][0] == 0:\n",
        "                    frame_index = video_indexes[indx][i+5]\n",
        "                    #print(frame_index)\n",
        "                    frame_path0 = frames_path + 'frame_' + str(int(frame_index)-1) + '.png'\n",
        "                    frame_path = frames_path + 'frame_' + str(frame_index) + '.png'\n",
        "                    #print(frame_path0)\n",
        "                    # read images\n",
        "                    img1 = cv2.imread(frame_path0)  \n",
        "                    img2 = cv2.imread(frame_path) \n",
        "\n",
        "                    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "                    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                    #sift\n",
        "                    sift = cv2.xfeatures2d.SIFT_create()\n",
        "                    #BRISK\n",
        "                    brisk = cv2.BRISK_create()\n",
        "\n",
        "                    keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\n",
        "                    keypoints_2, descriptors_2 = brisk.detectAndCompute(img2,None)\n",
        "\n",
        "                    print(len(keypoints_2))\n",
        "                    #feature matching\n",
        "                    #bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
        "\n",
        "                    #matches = bf.match(descriptors_1,descriptors_2)\n",
        "                    #print(len(matches))\n",
        "                    #matches = sorted(matches, key = lambda x:x.distance)\n",
        "                    #print(len(matches))\n",
        "                    #print(len(matches)/len(keypoints_1))\n",
        "                    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    \n",
        "                    pred_file.write(str(frame_index) + '\\n')\n",
        "pred_file.close()\n",
        "\n",
        "# delete files used for process\n",
        "os.remove('frames.txt')\n",
        "shutil.rmtree('video_frames/')\n",
        "\n",
        "print('Predictions complete !!!')\n",
        "print('Frames that are part of shot boundaries are listed in file the directory path predictions/' + pred_text_file_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVJqttoVbqIJ"
      },
      "source": [
        "import cv2\n",
        "img1 = cv2.imread('/content/messi_3.jfif')  \n",
        "img2 = cv2.imread('/content/messi_4.jfif') \n",
        "\n",
        "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                    #sift\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\n",
        "keypoints_2, descriptors_2 = sift.detectAndCompute(img2,None)\n",
        "\n",
        "print(len(keypoints_1))\n",
        "                    #feature matching\n",
        "bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
        "\n",
        "matches = bf.match(descriptors_1,descriptors_2)\n",
        "print(len(matches))\n",
        "                    #matches = sorted(matches, key = lambda x:x.distance)\n",
        "                    #print(len(matches))\n",
        "                    #print(len(matches)/len(keypoints_1))\n",
        "                    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcEfJOIY0bdx"
      },
      "source": [
        "#CHANGES IN CODE FOR FALSE PREDICTION REMOVAL\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import numpy as np\n",
        "from snippet import getSnippet\n",
        "from math import floor\n",
        "from transition_network import TransitionCNN\n",
        "from utilities import normalize_frame, print_shape\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from TestVideo import TestVideo, return_start_and_end\n",
        "from moviepy.editor import VideoFileClip\n",
        "from video_processing import six_four_crop_video\n",
        "from PIL import Image\n",
        "import cv2 \n",
        "#import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "\n",
        "# command line arguments --> file name, video_file_name, gpu or cpu \n",
        "\n",
        "\n",
        "# first decompose the video to frames\n",
        "# place the video to be detected into the directory \n",
        "\n",
        "video = sys.argv[1]\n",
        "pred_text_file_name = sys.argv[2]\n",
        "\n",
        "\n",
        "text_file = 'frames.txt'\n",
        "\n",
        "print('decomposing video to frames this may take a while  for large videos :) .....')\n",
        "frames_path = 'video_frames/'\n",
        "os.makedirs('video_frames/', exist_ok=True)\n",
        "os.makedirs('predictions/', exist_ok=True)\n",
        "\n",
        "vid = VideoFileClip(video)\n",
        "vid = six_four_crop_video(vid)\n",
        "\n",
        "frames = [frame for frame in vid.iter_frames()]\n",
        "\n",
        "f = open(text_file, 'w+')\n",
        "\n",
        "for j, frame in enumerate(frames):\n",
        "        frame_path = frames_path + 'frame_' + str(j+1) + '.png'\n",
        "        im = Image.fromarray(frame)\n",
        "        im.save(frame_path)            \n",
        "        f.write(frame_path + '\\n')    \n",
        "\n",
        "print('frame decomposition complete !!! ')\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "#load model\n",
        "model = TransitionCNN()\n",
        "model.load_state_dict(torch.load('shot_boundary_detector_even_distrib.pt'))\n",
        "model.to(device)\n",
        "\n",
        "prediction_text_file = 'predictions/' + pred_text_file_name \n",
        "\n",
        "pred_file = open(prediction_text_file, 'w+')\n",
        "\n",
        "print('computing predictions for video', video, '...................' )\n",
        "\n",
        "test_video = TestVideo('frames.txt', sample_size=100, overlap=9)\n",
        "test_loader = DataLoader(test_video, batch_size=1, num_workers=1)\n",
        "\n",
        "video_indexes = []\n",
        "vals = np.arange(test_video.get_line_number())\n",
        "length = len(test_video)\n",
        "\n",
        "for val in range(length):\n",
        "    s,e = return_start_and_end(val)\n",
        "    video_indexes.append(vals[s:e])\n",
        "\n",
        "for indx, batch in enumerate(test_loader):\n",
        "        batch.to(device)\n",
        "        batch = batch.type('torch.cuda.FloatTensor')\n",
        "        predictions = model(batch)\n",
        "        predictions = predictions.argmax(dim=1).cpu().numpy()\n",
        "        for idx, prediction_set in enumerate(predictions):\n",
        "            for i, prediction in enumerate(prediction_set):\n",
        "                if prediction[0][0] == 0:\n",
        "                    frame_index = video_indexes[indx][i+5]\n",
        "                    #print(frame_index)\n",
        "                    frame_path0 = frames_path + 'frame_' + str(int(frame_index)-1) + '.png'\n",
        "                    frame_path = frames_path + 'frame_' + str(frame_index) + '.png'\n",
        "                    #print(frame_path0)\n",
        "                    # read images\n",
        "                    img1 = cv2.imread(frame_path0)  \n",
        "                    img2 = cv2.imread(frame_path) \n",
        "\n",
        "                    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "                    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                    #sift\n",
        "                    #sift = cv2.xfeatures2d.SIFT_create()\n",
        "                    surf = cv2.xfeatures2d.SURF_create()\n",
        "\n",
        "                    #keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)\n",
        "                    keypoints_2, descriptors_2 = surf.detectAndCompute(img2,None)\n",
        "                    \n",
        "                    print(len(keypoints_2))\n",
        "                    #feature matching\n",
        "                    #bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
        "\n",
        "                    #matches = bf.match(descriptors_1,descriptors_2)\n",
        "                    #print(len(matches))\n",
        "                    #matches = sorted(matches, key = lambda x:x.distance)\n",
        "                    #print(len(matches))\n",
        "                    #print(len(matches)/len(keypoints_1))\n",
        "                    if (len(keypoints_2) != 0):               \n",
        "                      pred_file.write(str(frame_index) + '\\n')\n",
        "                    \n",
        "pred_file.close()\n",
        "\n",
        "# delete files used for process\n",
        "os.remove('frames.txt')\n",
        "shutil.rmtree('video_frames/')\n",
        "\n",
        "print('Predictions complete !!!')\n",
        "print('Frames that are part of shot boundaries are listed in file the directory path predictions/' + pred_text_file_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAYsprsZQyri"
      },
      "source": [
        "#BRISK KEYPOINTS\n",
        "!python get_transition_frames_gpu.py /content/v10.mp4 pred2.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}